```{python imports, echo=False, results=False}

import pickle

import xgboost as xgb

import pandas as pd

import numpy as np

from TextMiningMachine.splitting import TrainTestDateSplitter

import math

import datetime

from TextMiningMachine.utils import generate_column_from_search

from TextMiningMachine.plot_methods import plot_regress_corr, plot_shap,plot_shap_univar,plot_cutoff_accuracy,plot_shap_multivar,plot_classification_metrics

import matplotlib.pyplot as plt

from sklearn.metrics import roc_curve, auc

from sklearn.model_selection import train_test_split

import seaborn as sns

import shap

import gc

gc.enable()

import random

from scipy import sparse

from sklearn.metrics import accuracy_score

import pylab as pl

import sys



```

```{python data, echo=False,results = 'hidden'}

if __name__ == '__main__':

    ## around this point, the build_markdown code will insert in the call for the model_bundle_group and target column
    sys.path.insert(0, 'C:\PyProjects\Hourly_Predictions')
    target_col='MetSIRS4_4hr_24'
    model_name='MetSIRS4_4hr_24'
    model_bundle_group_filepath = 'Hourly_Predictions_bundle.p'
    data = pd.read_pickle('data/raw_data.p')
    date_based_split = True
    train_test_prop = 0.75
    cut_date = '2017-01-19'
    date_col = 'REPORTING_TIME'

    test_cut_date = data[date_col].sort_values().reset_index(drop=True)[int(data.shape[0] *.80)]
    keep_inds = np.where(data[date_col]<test_cut_date)[0]


    data = data.iloc[keep_inds,:].reset_index(drop=True)

    data = data[np.isfinite(data[target_col])].reset_index(drop=True)

    if date_based_split and cut_date is not None:

        cut_date = datetime.datetime.strptime(cut_date, "%Y-%m-%d").date()



    # load transform

    with open(model_bundle_group_filepath, 'rb') as f:

        mgb = pickle.load(f)



    mb = mgb.model_dict.get(model_name)



    # transform data into features
    features = sparse.load_npz('data/features.npz')
    #features = mb.trans.transform(data)
    features = sparse.csr_matrix(features)
    features = features[keep_inds,:]


    # these are in cases where the feature set was selected using feature selection. i.e len(model_features)!=len(transform)

    if len(mb.trans.feature_names) != len(mb.model.feature_names):

        # iterate through each of the features in the model and find the index associated with that feature in the transform.feature_names

        feature_inds = [mb.trans.feature_names.index(model_feature) for model_feature in mb.model.feature_names]

        features = sparse.csc_matrix(features)

        features = features[:,feature_inds]

    else:

        feature_inds = range(len(mb.trans.feature_names))

    features.feature_names = mb.model.feature_names



    if not date_based_split:

        from sklearn.model_selection import train_test_split

        # split the data

        train, test, y_train, y_test = train_test_split(features, data[target_col], test_size=(1-train_test_prop), random_state=0)

        # format the training and test sets

        train = xgb.DMatrix(train, label=y_train, feature_names=[mb.trans.feature_names[i] for i in feature_inds])

        test = xgb.DMatrix(test, label=y_test, feature_names=[mb.trans.feature_names[i] for i in feature_inds])

    else:

        # cut by date

        from TextMiningMachine.splitting import TrainTestDateSplitter

        import datetime

        s = TrainTestDateSplitter()



        # if the cut_date for the train/test split, it will find the date that splits the data with the specified train_test_prop

        if cut_date==None:

            cut_date = data[date_col].sort_values()[int(data.shape[0] * train_test_prop)]

            print('No cut_date was specified, a cut_date of '+str(cut_date)+ 'is being used due to the proportion of training data being '+str(train_test_prop))

        test, train, y_test, y_train = s.split(data, target_col, date_col, cut_date, features=xgb.DMatrix(features, feature_names=[mb.trans.feature_names[i] for i in feature_inds]))



    preds_test = mb.model.predict(test)

    preds_train = mb.model.predict(train)

    y_train=train.get_label()

    y_test = test.get_label()





```





## Objective to Predict MetSIRS4_4hr_24
#### Modeling Approach



This problem was modeled using gradient boosted decision trees constructed via the XGBoost package in Python. The optimal parameters for this model were found using a random and sequential search algorithm over the parameter space to find the values that optimized the objective function. All classification models are built using two objective functions, to maximize AUC and minimize misclassification log-loss, and all regression models are built using two objective functions, to maximize R^2 and RMSE.  L1 and L2 regularization grid search was used to optimize the model.



#### Training/Test Split



```{python, echo=False, results = 'tex'}

print('The proportion of training/test data used in constructing the model was: '+str(int(np.round(train.num_row()/(train.num_row()+test.num_row()),2)*100))+'% / '+

                                                                                  str(int(np.round(test.num_row()/(train.num_row()+test.num_row()),2)*100))+'%. ')

print('The total number of rows in the dataset : '+str(test.num_row()+train.num_row())+'. ')

print('The number of training rows : ', train.num_row(),'. ')

print('The number of testing rows : ', test.num_row(),'. ')

```





#### Variables Considered



The following list of variables were used to build the model.  The text columns are semi-structured and put into a bag of words model.  Categorical columns were one hot encoded.  All numeric variables were zero/median imputed when missing.



```{python, variables, echo=False, width=850}

# training variables

for col_type in mb.trans.col_dict.keys():

    print(col_type+': ' + str(mb.trans.col_dict.get(col_type))+'\n')

# final number of features

print('The total number of features included in the model were: ', str(len(mb.model.feature_names)),'.\n')

```









#### AUC of the ROC Curve

In a ROC curve, the true positive rate (Sensitivity) is plotted in function of the false positive rate (Specificity) for different cut-off values. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold.



```{python, AUC_Plot, echo=False}





preds_test = mb.model.predict(test)

fpr, tpr, _ = roc_curve(y_test, preds_test)

preds_train = mb.model.predict(train)

fpr_t, tpr_t, _ = roc_curve(y_train, preds_train)



# Calculate the AUC

roc_auc = auc(fpr, tpr)

roc_auc_t = auc(fpr_t, tpr_t)

#print('ROC AUC: %0.2f' % roc_auc)

plt.figure()

plt.plot(fpr, tpr, label='Test AUC  %0.2f' % roc_auc)

plt.plot(fpr_t, tpr_t, label='Train AUC %0.2f' % roc_auc_t)

plt.plot([0, 1], [0, 1], 'k--')

plt.xlim([0.0, 1.0])

plt.ylim([0.0, 1.05])

plt.xlabel('False Positive Rate')

plt.ylabel('True Positive Rate')

plt.title('ROC Curve')

plt.legend(loc="lower right")

plt.show()

```



#### Sensitvity and Specificity Curves

Sensitivity (true positive rate) is the proportion of cases where the model predicts positive given that the target variable is present. Specificity (true negative rate) is the proportion of cases where the model predicts negative given that the target variable is absent.



```{python,Sen_Spec_Curve,echo=False, evaluate=True,results = 'hidden'}

fpr, tpr, thresholds = roc_curve(y_test, preds_test)

roc_auc = auc(fpr, tpr)

#print("Area under the ROC curve : %f" % roc_auc)

i = np.arange(len(tpr)) # index for df

roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)})



roc.iloc[(roc.tf-0).abs().argsort()[:1]]



# Plot tpr vs 1-fpr

fig, ax = pl.subplots()

pl.plot(roc['tpr'], color='orange', label='sensitivity')

pl.plot(roc['1-fpr'], color = 'blue', label='specifity')

pl.xlabel('Cut Off')

pl.ylabel('Sensitiviy | Specificity')

pl.title('Sensitivity, Specificity vs Cut Off')

ax.set_xticklabels([])

plt.legend()

```

##### Predicted Probability Distribution

The distribution of model prediction is shown below. The red line represents the proportion of cases where the target variable is present.



```python Density Plots, echo = False

fig, ax = plt.subplots(figsize=(4,4))

sns.distplot( preds_test, hist=False)

plt.axvline(sum(y_test)/len(y_test), color='r')

plt.title("Distribution of Predicted Probablities", loc='right')

plt.xlabel('Probablity')

plt.ylabel('Density')

plt.show()





```



#### Accuracy vs. Cutoff Plot

This plot displays the trade-off between cutoff value and accuracy. Optimal cutoff here is found by maximizing accuracy of model. This is not ideal with problems that have class imbalance; however, optimal cutoffs can be chosen when intervention costs are known.



```{python, accuracy_vs_cutoff, echo = False}

cut = plot_cutoff_accuracy(y_train, y_test, preds_train, preds_test)

```





#### Model Metrics using Using Specified Cutoff

Sensitivity (true positive rate) is the proportion of cases where the model predicts positive when the target variable is present. Specificity (true negative rate) is the proportion of cases where the model predicts negative when the target variable is absent. Positive predictive value is the proportion of cases where the target variable is present when the model predicts positive. Negative predictive value is the proportion of cases where the target variable is absent when the model predicts negative.



#### Classification Plot using Optimal Cutoff Above

```{python, fig = True, width=850, name='model_metrics', echo = False,evaluate=True}

plot_classification_metrics(y_train, y_test, preds_train, preds_test, cut)

```





#### Variable Importance

The variable importance plots display the most important features by their information gain, which is a measurement that sums up how much 'information' a feature gives about the target variable. Information gain measures the reduction in entropy, or uncertainty, over each of the times that the given feature is split on.



```{python, variable_importance, width=850, echo=False,results = 'hidden'}

mb.plot_feature_importance(24)

```





##### SHAP Univariate Plots



SHapley Additive exPlanations(SHAP) is an approach to explain the output of machine learning models. SHAP assigns a value to each feature for each prediction (i.e. feature attribution); the higher the value, the larger the feature's attribution to the specific prediction. In cases of classification, a positive SHAP value indicates that a factor increases the value of the model's prediction(risk), whereas a negative SHAP value indicates that a factor decreases the value of the model's prediction. The sum of SHAP values over all features will approximately equal the model prediction for each observation. In the following plots blue points signify negative SHAP values(values of the feature associated with reducing risk for sepsis), red points have positive SHAP values(values of the feature associated with increasing risk for sepsis), and yellow points are values for which the feature attributes little to the prediction for Sepsis.



```{python shap_data, echo=False, evaluate=True, results = 'hidden'}

if __name__ == '__main__':

    ## generate samples for sensitivity plots( this needs to go before the importance_frame code )

    samp_size=10000 if data.shape[0]>=10000 else data.shape[0]

    rand_inds = np.sort(random.sample(range(data.shape[0]),samp_size))

    features = sparse.csr_matrix(features)

    shap_vals = mb.model.predict(xgb.DMatrix(features[rand_inds,:],feature_names=mb.model.feature_names),pred_contribs=True)

    samp_df = pd.DataFrame(features[rand_inds,:].todense())

    samp_df.columns=mb.model.feature_names

```



```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
shap_feat = 'MetSIRS_WBC'
plt.figure()
plot_shap_univar(shap_feat, shap_vals, samp_df,feature_names = samp_df.columns)
```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
shap_feat = 'MaxRR8'
plt.figure()
plot_shap_univar(shap_feat, shap_vals, samp_df,feature_names = samp_df.columns)
```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
shap_feat = 'MaxRR24'
plt.figure()
plot_shap_univar(shap_feat, shap_vals, samp_df,feature_names = samp_df.columns)
```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
shap_feat = 'MEWS_Temp'
plt.figure()
plot_shap_univar(shap_feat, shap_vals, samp_df,feature_names = samp_df.columns)
```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
shap_feat = 'MinTemp8'
plt.figure()
plot_shap_univar(shap_feat, shap_vals, samp_df,feature_names = samp_df.columns)
```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
shap_feat = 'MinTemp24'
plt.figure()
plot_shap_univar(shap_feat, shap_vals, samp_df,feature_names = samp_df.columns)
```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
shap_feat = 'CPM S16 R AS SC RASS (RICHMOND AGITATION-SEDATION SCALE) (TRANSFORMED)'
plt.figure()
plot_shap_univar(shap_feat, shap_vals, samp_df,feature_names = samp_df.columns)
```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
shap_feat = 'MinHR24'
plt.figure()
plot_shap_univar(shap_feat, shap_vals, samp_df,feature_names = samp_df.columns)
```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
shap_feat = 'SODIUM'
plt.figure()
plot_shap_univar(shap_feat, shap_vals, samp_df,feature_names = samp_df.columns)
```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
shap_feat = 'TEMPERATURE'
plt.figure()
plot_shap_univar(shap_feat, shap_vals, samp_df,feature_names = samp_df.columns)
```


##### SHAP Summary Plot Over Categorical/Text Columns

The following plot display the effect of the top text/category values on the Model's predictions. Red data points represent when the text/column feature is observed in a patient's records, whereas the blue represents when the feature is not observed.

```{python, fig = True,width=850, name = 'SHAP Cat/Text Summary',echo=False, evaluate=True,results = 'hidden'}



# this section plots the SHAP summary plot using only categorical&text features from the important features above





cat_text_features = ['CPM S16 R INV O2 DEVICE.room air', 'CPM S16 R INV O2 DEVICE.nasal cannula', 'CPM S16 R INV O2 DEVICE.ventilator', 'CPM S16 R INV O2 DEVICE.tracheostomy collar', 'CPM S16 R INV O2 DEVICE.mechanical ventilator', 'FIO2  ARTERIAL', 'CPM S16 R INV O2 DEVICE.nasal cannula with humidification', 'PO2 (CORR)  ARTERIAL', 'CPM F12 ROW TUBE FEEDING INTAKE (ML) (ADULT  NICU  OB  PEDIATRIC)', 'BILIRUBIN  TOTAL', 'PT TEMP (CORR)  ARTERIAL', 'PCO2 (CORR)  ARTERIAL', 'pH (CORR)  ARTERIAL', 'TOTAL CO2  ARTERIAL', 'BICARB  ARTERIAL', 'CPM S16 R AS PAIN RATING (0-10)  REST', 'O2 SAT  ARTERIAL', 'BASE  ARTERIAL', 'GLUCOSE  WHOLE BLOOD', 'UREA NITROGEN  BLOOD (BUN)']
feat_inds = [mb.model.feature_names.index(cat_text_features[i]) for i in range(len(cat_text_features))]
shap.summary_plot(shap_vals[:, feat_inds], samp_df.iloc[:, feat_inds],show=False)
plt.subplots_adjust(left=.3, bottom=None, right=.95, top=None, wspace=None, hspace=None)
plt.show()
```